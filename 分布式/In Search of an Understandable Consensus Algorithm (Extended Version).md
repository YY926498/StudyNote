# In Search of an Understandable Consensus Algorithm (Extended Version)

Raft的特点：

- strong leader：Raft相比其他共识算法具有更强烈的领导力。日志条目只从领导者到其他服务器，简化了复制日志的管理。
- Leader election：Raft使用随机计时器选出Leader
- Membership changes：允许在配置更改期间仍然能够正常运行

## 复制状态机

复制状态机用于解决分布式系统中的各种容错问题。例如具有单个集群领导者的大型系统，如GFS、HDFS、RAMCloud。通常使用单独的复制状态机来管理领导选举，并存储当领导者宕机时必须要保留下来的配置信息。采用复制状态机的如Chubby和ZooKeeper。

复制状态机通常使用复制的日志实现。每个服务器存储一系列命令的日志，其状态机按顺序执行该命令。每个日志都包含相同顺序的相同命令，因此每个状态机处理相同的命令序列。由于状态机是确定的，因此每个机器具有相同的状态和相同的输出序列。

共识算法的功能是确保复制的日志一致。服务器上的公式模块是从客户端接收命令并将其添加到日志中。并与其他服务器上的公式模块通信，确保即使某些服务器下线每个日志最终顺序是相同的。一旦正确复制命令，每个服务器的状态机在日志顺序中处理它们，输出返回给客户端。

共识算法通常具有以下属性：

- 在所有非拜占庭条件下，确保安全，包括网络延迟、分区和数据包丢失、复制和重新排序
- 只要大多数服务器在工作，并且能够互相通信和与客户端通信，就可以完全发挥作用。服务器如果因为停机而掉线，稍后可以从硬盘上读取状态重新加入集群。
- 不依赖时间来确保日志的一致性，这是因为错误的时间和极端的消息延迟在最坏的情况下可能导致可用性问题。
- 通常情况下，只要集群中的大多数对单个RPC做出响应，命令就可以完成，少数慢服务器不影响整体系统性能。

## Paxos的问题

Paxos定义了一个协议，该协议能够将单个决策（如单个复制的日志条目）达成一致。这个子集称为单一法令Paxos。然后Paxos将该协议的多个实例组合起来，以促进一系列决策。Paxos确保安全性和活动性，并且支持集群成员的更改。

Paxos有两个明显的缺点：

- Paxos非常难以理解。假设Paxos的不透明性源于它选择单一法令子集作为其基础。单一法令的Paxos是密集而微妙的：分为两个阶段，没有简单的直观解释，不能独立理解。因此，很难直观地理解为什么单一法令协议能起作用。multi-Paxos的组合规则显著增加了复杂性和微妙性。
- 没有为构建实际实现提供良好基础。一个原因是，对于multi-Paxos没有得到广泛认可的算法。

另外由于单一法令的分解的原因，Paxos架构很难构建。例如，独立地选择一组日志条目，然后将它们合并成一个连续的日志，并没有什么好处，只是增加了复杂性。围绕日志设计系统更简单、更高效，其中新条目接受约束的顺序依次添加。另一个问题是Paxos在其核心使用了Peer-to-Peer。这在一个只有一个决策的简化世界中是有意义的，但是很少有实际系统使用这种方法。如果必须要做一系列的决定，那么首先选举一个领导，然后让领导协调决定，这样做比较简单快捷。

”Paxos算法的描述与真实系统的需求之间存在显著差距，最终的系统将基于一个未经证实的方案。“

## 设计可理解性

设计Raft时，必须为系统构建提供一个完整而实用的基础，从而大大减少开发人员的设计工作量；必须在所有条件下都是可行的，并且在典型操作条件下是可用的；对于一般的操作，必须是有效的。最重要也是最困难的挑战是理解。必须能够让大量的观众舒服地理解算法。

采用如下两种方法来分析是否便于理解：

- 问题分解方法：只要可能，将问题划分成可以相对独立地解决、解释和理解的独立部分。
- 通过减少要考虑的状态的数量来简化状态空间，使系统更加一致并尽可能消除不确定性。具体来说，日志不允许中间有间隔，并且Raft限制了日志之间可能不一致的方式。尽管在大多数情况下，试图消除非确定性，但在某些情况下，非确定性实际上提高了可理解性。随机化方法引入了非确定性，倾向于通过以类似的方式处理所有可能的选择来减少状态空间。

## Raft一致性算法

Raft是一种管理复制日志的算法。Raft首先选出一位leader，然后赋予leader管理复制日志的完全责任来实现共识。leader接受来自客户端的日志条目，将它们复制到其他服务器上，并告诉服务器何时可以安全地将日志条目应用到它们的状态机。

存在单一leader简化了对复制日志的管理。例如，leader可以决定在日志中放置新条目的位置，而不需要咨询其他服务器，数据以一种简单的方式从leader流到其他服务器。单一leader可以宕机或与其他服务器断开连接，此时，将选举一个新的leader。

鉴于leader方法，Raft将共识问题分解为三个相对独立的子问题：

- leader选举：当现有leader失败时，必须选出新的leader
- 日志复制：leader必须接受来自客户端的日志条目，并在整个集群中复制它们，迫使其它日志与自己的一致。
- 安全：Raft的关键安全属性是状态机安全属性：如果任何服务器对其状态机应用了特定的日志条目，那么其他服务器就不能对相同的日志索引应用不同的命令。

#### 状态

所有服务器上的持久状态（在响应RPCs之前对稳定存储进行更新）：

- `currentTerm`：服务器看到的最新的term。第一次初始化为0，单调增加
- `votedFor`：当前任期内获得投票的`candidatId`（候选者Id），如果没有则为null
- `log[]`：日志条目，每个条目包含状态机的命令，以及leader接收到这条目时的term（第一个索引是1）

所有服务器上的不稳定状态：

- `commitIndex`：已知提交的最高日志项的索引（初始化为0，单调增加）
- `lastApplied`：应用于状态机的最高日志条目的索引（初始化为0，单调地增加）

leader的不稳定状态（初始化后的选举）：

- `nextIndex[]`：对于每一个服务器，发送到该服务器的下一个日志条目的索引（初始化为leader最后一个日志索引+1）
- `matchIndex[]`：对于每一个服务器，已知要在服务器上复制的最高日志条目的索引（初始化为0，单调地增加）

#### RequestVote RPC

由候选人调用，收集选票

参数：

- term：候选人的term
- candidateId：请求投票的候选人
- lastLogIndex：候选人最后一个日志条目的索引
- lastLogTerm：候选人最后一个日志条目的term

结果：

-  term：currentTerm，以便候选人更新自身
- voteGranted：true表示候选人获得了选票

接收实现逻辑：

1. 如果term<currentTerm，返回false（如果候选人比我的term还小，直接拒绝它）
2. 如果votedFor==null或者候选人的日志至少比接受者新，则给它投票要投票的人的日志不能比我还少，Raft总想选择日志最多的节点作为Leader）

#### AppendEntries RPC

由leader调用来复制日志条目，也可用于心跳

参数：

- term：leader的term
- leaderId：所有follower可以重定向的客户端
- prevLogIndex：紧挨着新条目的日志条目的索引
- prevLogTerm：prevLogIndex的term
- entries[]：要存储的日志条目（空为心跳，可能为了效率发送多个）
- leaderCommit：leader的commitIndex

结果：

- term：currentTerm，由leader更新
- success：如果follower包含匹配prevLogIndex和prevLogTerm，则为true

接收实现逻辑：

1. 如果term<currentTerm，则返回false
2. 如果prevLogIndex位置的日志所属的任期和prevLogTerm不匹配，返回false（Raft要求，相同位置有相同日志）
3. 如果一个现有的条目和一个新的条目冲突(相同的索引但是不同的任期)，删除现有的条目和它后面的所有条目(5.3)（这里Raft发现问题之后，强制同步日志）
4. 追加任何尚未在日志中出现的新条目
5. 如果leaderCommit > commitIndex，设置 commitIndex = min(leaderCommit, index of last new entry)（有可能有节点没有复制完，那么全局的提交位置是最小值）

#### Rules for servers

所有节点适用的规则：

- 如果commitIndex>lastApplied：增加lastApplied，将log[lastApplied]应用到状态机
- 如果RPC请求或者响应中的term  T>currentTerm，设置currentTerm=T，并将自身转换为follower。（收到的RPC请求或响应中包含的term比我当前term大，说明自身不是leader，转为follower）。

followers：

- 响应候选人和领导的要求
- 如果选举超时，没有收到当前leader的AppendEntries RPC（超过选举超时时间没有收到心跳就转为候选人，目的为了重新开始选举，长期收不到心跳可能是leader宕机）或授予候选人投票：转换为候选人

Candidates：

- 转为候选人，开始选举
  - 增加currentTerm
  - 给自己投票
  - 重置选举定时器
  - 发送RequestVote RPCs到所有服务器
- 如果从大多数服务器获得票数：成为leader
- 如果从新领导着接收到AppendEntries RPC：转换为follower
- 如果选举超时，开始新的选举

leaders：

- Upon election：发送初始空的AppendEntries RPCs（心跳）到每个服务器；在空闲期间重复，防止选举超时
- 如果从客户端接收到命令：追加条目到本地日志，在条目应用到状态机后进行响应
- 如果follower的最后一个日志条目（last log index）>= nextIndex：发送AppendEntries RPC，其中日志条目从nextIndex开始
  - 如果成功：更新follower的nextIndex和matchIndex
  - 如果由于日志不一致导致AppendEntries失败：递减nextIndex并重试
- 如果存在一个N，使得N>commitIndex，并且matchIndex数组中大多数元素都满足matchIndex[i]>=N，并且log[N].term==currentTerm：设置commitIndex=N（当前term下，大多数位置N后面的日志条目都已经复制到了其他节点上，那么就可以认为N位置之前的都可以提交了。注意：复制日志条目和提交不同，有可能存在复制了日志条目，但是最后没有设置commitIndex=N的情况）

Raft重要属性：

- Election Safety：在给定的term中最多只能选举一位leader
- Leader Append-Only：leader从不覆盖或删除日志中的条目，只添加新条目
- Log Matching：如果两个log中的一个条目具有相同的term和index，那么它们之前的log应该是一致的
- Leader Completeness：如果一个log条目在一个term中被committed，那么它会出现在所有更高term的leader的log中。
- State Machine Safety：如果一个server已经把一个日志条目应用到它的状态机，那么没有其他的server可以应用一个具有同样的term和index的不同日志条目。

### Raft基础知识

一个Raft集群包含多个服务器，5是一个典型数字，允许系统容忍两次故障。在任何给定的时间，每个服务器都处于三种状态之一：leader、follower或candidate。在正常运行中，只有一个leader，其他所有服务器都是follower。follower是被动的：自己不发出任何请求，只是回应leader和candidate的请求。leader处理所有客户端请求（如果客户端联系到一个follower，follower将它重定向到leader）。candidate用于选举新leader。

Raft将时间划分为任意长度。term用连续整数进行编号。每个term以选举开始，其中一名或多名candidate试图成为leader。如果candidate赢得了选举，那么将在剩下的term内担任leader。某些情况下，选举导致分裂投票。此时，term将会以没有leader结束，新的term即将开始。Raft确保在一个给定的term内最多有一个leader。

不同的服务器可能会在不同的时间观察到term之间的转换，在某些情况下，服务器可能不会观察到选举甚至整个term。term在Raft中充当逻辑时钟，允许服务器检测过时的信息，比如过时的leader。每个服务器存储一个currentTerm，随着时间单调地增加。currentTerm在服务器通信时进行交换，如果一个服务器的currentTerm小于另一个服务器的，那么将currentTerm更新为较大的值。

如果一个candidate或leader发现自身的term过时，立即恢复到follower状态。如果一个服务器接收到一个过期term的请求，拒绝该请求。

Raft服务器使用RPC进行通信，基本共识算法只需要两种RPC：RequestVote RPC在选举期间由candidate发起，AppendEntries RPC由leader发起，以复制日志条目并提供一种心跳的形式。另外添加用于在服务器之间传输快照的RPC。

### Leader election

Raft使用心跳机制来触发leader election。当服务器启动时，默认为follower。只要服务器从leader或candidate接收到有效的RPC，就保持在follower。leader定期向所有followers发送心跳（没有日志条目的AppendEntries），维护它们的权限。如果follower在一段成为选举超时的时间内没有收到任何通信，会假设没有可行的leader，并开始选举一个新的leader。

为了开始选举，follower会增加其term，并将自身转为candidate状态。然后为自己投票，并并行地向集群中的每个其他服务器发送RequestVote RPC。candidate会一直维持在这个状态，直到发生以下三件事的一种：a)赢得选举；b)另一个服务器成为新的leader；c)一段时间过去没有任何服务器成为leader。

如果一个candidate在同一个term内获得了整个集群中大多数服务器的选票，那么就赢得了选举。每个服务器在一个任期内最多给投票给一个candidate，以先到先得的方式。多数决定原则确保最多一个candidate可以在一个特定的任期内赢得了选举（选举安全属性）。candidate一旦赢得选举，就会成为领袖。然后向其他所有服务器发送心跳信息，以获得其权威，防止新的选举。

在等待投票时，candidate可能从另一个服务器收到一个自称leader的AppendEntries RPC。如果leader的term（包括RPC）至少和candidate的当前term一样大，那么candidate认为leader是合法的，并转换为follower。如果RPC中的term小于candidate的当前term，那么candidate拒绝RPC并继续维持在candidate。

第三种可能的结果是，一个candidate既没有赢得选举，也没有输掉选举：如果选多follower同时成为candidate，选票可能被分割，因此没有一个candidate获得多数票。当发生这种情况时，每个candidate都将超时并通过增加term并启动另一轮RequestVote RPC来开始新的选举。然而，如果没有额外的措施，分裂投票可能会无限期地重复。

Raft使用随机的选举超时来确保很少出现分裂的投票，并且能够快速解决。为了防止选票分裂，选举超时是在固定的时间间隔（如150-300ms）中随机选择的。同样的机制也用于处理分裂的投票。每个candidate在选举开始时重新启动它的随机选举超时，并在开始下一个选举之前等待该超时过去，这降低了在新选举中再次出现分裂选票的可能性。

### 日志复制

一旦一个leader当选，就开始服务客户端的请求。每个客户端请求都包含一个由由复制状态机执行的命令。leader将该命令作为一个新条目添加到其日志中，然后并行地向每个其他服务器发出AppendEntries RPC，复制该条目。当条目被安全复制时，leader将条目应用到它的状态机，并将执行的结果返回给客户端。如果follower崩溃或运行缓慢，或者如果网络数据包丢失，leader将无限期地重试AppendEntries RPC（即使已经对客户端发出响应之后），直到所有follower最终存储所有日志条目。

每个日志条目存储一个状态机命令，以及leader接收条目时的term。日志条目中的term用于检测日志之间的不一致性，并确保某些属性。每个日志条目有一个整数索引，用来标识其在日志中的位置。

leader决定何时将日志条目应用到状态机是安全的，这样的条目称为提交。Raft保证提交的条目是持久的，最终由所有可用的状态机执行。一旦创建条目的leader在大多数服务器上复制了一个日志条目，这个日志条目就被提交。这会在leader的日志中提交所有之前的条目，包括之前的leader创建的条目。leader跟踪它知道要提交的最高索引，并将该索引包含在未来的AppendEntries RPC中（包括心跳），以便其他服务器最终发现。一旦follower知道一个日志条目被提交，就将这个条目应用到它的本地状态机（按日志顺序）。

Raft日志机制维护不同服务器上日志之间的高一致性。不仅简化了系统的行为，使其更易于预测，而且是确保安全的一个重要组成部分。Raft维护以下属性：

- 如果不同日志中的两个条目具有相同的索引和term，则存储相同的命令。
- 如果不同日志中的两个条目具有相同的索引和term，则之前所有条目的日志是相同的。

第一个属性源于：在特定的term中，一个leader最多创建一个带有给定日志索引的条目，而日志条目永远不会改变它们在日志中的位置。第二个属性由AppendEntries RPC执行的一个简单的一致性检查来保证。当发送AppendEntries RPC时，leader在其日志中包含条目的索引和term，该条目紧接在新条目之前。如果follower在其日志中没有找到相同索引和term的条目，将拒绝新的条目。因此，每当AppendEntries RPC成功返回时，leader就知道follower的日志通过新条目与它自己的日志是相同的。

在正常的操作过程中，leader和follower的日志保持一致，所以AppendEntries一致性检查不会失败。但是leader宕机可能会使日志不一致（旧的leader可能没有完全复制其日志中的所有条目）。这些不一致会导致一系列leader和follower崩溃。follower可能会缺少leader上存在的条目，也可能有额外的leader上不存在的条目，或者两者都有。

在Raft中，leader通过强制follower的日志复制自身的日志来处理不一致性。这意味着follower日志中的冲突条目将被leader日志中的条目覆盖。

为了使follower的日志与自己保持一致，leader必须找到两个日志一致的最新日志条目，删除follower日志中在该点之后的所有条目，并将该点之后leader的所有条目发送给follower。所有这些操作都是为了响应AppendEntries RPC执行的一致性检查。leader为每一个follower维护一个nextIndex，这是leader将发送给那个follower的下一个日志条目的索引。当一个leader首次掌权时，将所有nextIndex初始化为其日志中最后一个索引之后的索引。如果follower的日志与leader的不一致，AppendEntries一致性检查将在下一次AppendEntries RPC中失败。拒绝后，leader将nextIndex递减，并重试AppendEntries RPC。

如果需要，可以优化协议减少被拒绝的AppendEntries RPC的数量。如果当拒绝一个AppendEntries请求时，follower可以包含冲突项的term和该term内的第一个索引。有了这个信息，leader就可以递减nextIndex来绕过该term中所有冲突的条目，每个有冲突项的term都需要一个AppendEntries RPC，而不是每一个条目一个RPC。

由于这种机制，leader在掌权时不需要采取任何特殊动作来恢复日志的一致性。开始只有正常的操作，当AppendEntries一致性检查失败时，日志自动收敛。leader从不覆盖或删除自身日志中的条目。

这个日志复制机制展示了一致性属性：只要大多数服务器都是正常的，Raft就可以接受、复制和应用新的日志条目。通常情况下，一个新条目可以用一轮RPC复制到集群的大部分，单一的慢follower不会影响性能。

### 安全

#### 选举限制

在任何基于leader的一致性算法中，leader最终必须存储所有提交的日志条目。Raft使用一种简单的方法，保证从选举的那一刻起，每个新的leader上提交的所有条目都存在，不需要将这些条目转移给leader。这表示日志条目只向一个方向流动，从leader到follower，leader从不覆盖它们日志中的现有条目。

Raft使用投票过程防止一个candidate赢得选举，除非其日志包含所有已提交的条目。candidate必须获得集群中的大多数才能被选中，这意味着每个提交的条目至少出现在其中的一个服务器上。如果candidate的日志至少和大多数的其他日志一样是最新的，那么将保存所有提交的条目。RequestVote RPC实现了这个限制：RPC包含关于candidate日志的信息，如果投票者自己的日志比candidate的日志更新，拒绝对其投票。

Raft通过比较日志中最后一个条目的索引和term来确定两个日志中哪个更新。如果日志中有带有不同term的最后一个条目，那么带有较大的term的日志是最新的。如果日志的term相同，索引大的更新。

#### 提交之前的条目

leader知道一旦当前条目存储在多数服务器上，就会提交该项。如果一个leader在提交一个条目之前崩溃，未来的leader将尝试完成这个条目的复制。然而，一旦前一个条目存储在大多数服务器上，leader不能立即断定它已经提交。仍然存在旧的条目存储在多数服务器上，但仍然可以被未来的leader覆盖。

为了消除该问题，Raft不通过副本来提交之前term的日志条目。只有来自leader当前term的日志条目被多数副本提交。一旦当前term中的一个条目以这种方式提交，那么由于日志匹配属性，所有之前的条目都将间接提交。在某些情况下，leader可以安全地断定已提交了一个较旧的日志条目（比如该条目存储在每个服务器上），但为了简单起见，Raft采用更保守的方法。

因为当leader复制前一个term的条目时，日志条目保留了它们原来的term。Raft的方法使得对日志条目进行推理变得更容易，因为随着时间的推移和跨日志保持相同的term。另外，新leader发送的来自以前term的日志条目更少。

#### 安全参数

假设term为T时的leader为$leader_T$，该leader提交一个日志条目，但是未来的新leader没有对应存储。假设未来的新leader所在的term为U。其中$leader_U$没有存储这个对应的条目。

1.  在$leader_U$选举时，其日志中不存在提交的条目（leader从不删除或覆盖条目）
2.  $leader_T$将条目复制到集群的多数，并且$leader_U$收到了集群的多数投票。因此，至少有一个投票既接受了$leader_T$的条目，又给$leader_U$投票。这个重叠的服务器就是关键。
3.  投票的服务器在投票选举$leader_U$前必须接受$leader_T$提交的条目，否则，将拒绝来自$leader_T$的AppendEntries请求（其当前term大于T）
4.  投票的服务器在投票给$leader_U$时仍然存储条目，因为每个介入的leader都包含条目，leader从不删除条目，follower只在与leader冲突时删除条目。
5.  投票的服务器将自己的票投给$leader_U$，所以$leader_U$的日志必须和投票人一样是最新的。
6.  首先，如果投票的服务器和$leader_U$的上一个term相同。那么$leader_U$的日志必须至少和投票的服务器一样长，所以它的日志包含投票服务器日志中的每个条目。

leader completeness属性：如果一个服务器将给定索引的日志条目应用到状态机，那么其他的服务器将不会为同一索引应用到不同的日志条目。当服务器将一个日志条目应用到它的状态机时，它的日志必须通过该条目与leader的日志相同，并且必须提交该条目。log completeness属性保证所有具有更大term的leader将存储相同的日志条目，因此应用后面term的索引的服务器将应用相同的值。因此，状态机的安全属性保持不变。

最后，Raft要求服务器按照日志索引顺序应用条目。结合状态机安全属性，这意味着所有服务器将以相同的顺序对其状态机应用完全相同的日志条目集合。

### follower和candidate崩溃

follower和candidate崩溃的处理方式相同。如果follower或candidate崩溃，那么发送给它的RequestVote和AppendEntries RPC失败。Raft通过反复尝试来处理这些失败。如果崩溃的服务器重新启动，则RPC将成功完成。如果服务器在完成RPC后但在响应之前崩溃，那么在重新启动后再次收到相同的RPC。Raft的RPC是幂等的，没有副作用。如果一个follower接收到一个已经在其日志中的条目的AppendEntries请求，将忽略新请求中的那些条目。

### 时间和可用性

Raft的安全性不依赖时间：系统不能仅仅因为某些事件的发生比预期的快或慢就产生不正确的结果。然而，可用性必然依赖于时间。

leader选举是Raft的一个方面。其中时间最关键。只要系统满足以下时序要求，Raft就能选出并保持一个稳定的leader：

$broadcastTime << electionTimeout <<  MTBF$

$broadcastTime$是服务器向集群中的每个服务器并行发送RPC并接收它们的响应所需的平均时间；$electionTimeout$为选举超时；$MTBF$是单个服务器发生故障之间的平均时间。广播时间应该比选举超时时间小一个数量级，以便leader能够可靠地发送防止follower开始选举所需的心跳信息；考虑到选举超时使用的随机方法，这种不平等也使得分裂选票不太可能。选举超时时间应该比$MTBF$小几个数量级，这样系统才能稳定运行。当leader崩溃时，系统将在选举超时时间内不可用。

广播时间和$MTBF$是底层系统的属性，而选举超时是我们选择的。Raft的RPC通常要求接收方将信息持久化到稳定的存储中，因此广播时间可能从0.5ms到20ms不等，这取决于存储技术。因此，选举超时可能在10ms到500ms之间。典型的服务器$MTBF$是几个月或更长时间，因此时间要求比较容易满足。

## 集群成员变更

在实践中，有时需要更改配置。虽然可以通过使整个集群脱机。更新配置文件，然后重新启动集群来完成，但这将使集群在转换期间不可用。此外，有操作错误的风险。

为了确保配置变更机制的安全，在交接期间不能出现两名leader同时当选的情况。但是，任何服务器直接从旧配置切换到新配置的方法都是不安全的。不可能一次以原子方式切换所有服务器，因此在转换期间集群可能会被分为两个独立的多数。

为了确保安全性，配置更改必须使用两阶段方法。在Raft中，集群首先切换到一个过渡配置，称之为联合共识，一旦达成联合共识，系统就会过渡到新的配置。联合共识包括了新旧配置：

- 在这两种配置中，日志条目被复制到所有服务器。
- 任一配置中的任何服务器都可以作为leader
- 协议（关于选举和日志提交）需要新旧两种结构中单独的多数

联合共识允许不同的服务器在不同的时间、不同的配置之间进行转换，而不影响安全性。此外，联合共识允许集群在整个配置更改过程中继续服务客户端请求。

集群配置使用复制日志中的特殊条目进行存储和通信。当leader收到将配置从$C_{old}$更改为$C_{new}$的请求时，它将联合共识的配置$C_{old,new}$存储为一个日志条目，并使用前面描述的机制复制该条目。一旦给定的服务器将新的配置条目添加到其日志中，它将为所有未来的决策使用该配置（服务器总是在其日志中使用最新的配置，无论该条目是否提交）。这意味着leader使用$C_{old,new}$确定$C_{old,new}$的日志条目何时被提交。如果leader崩溃，一个新的leader会被选为$C_{old}$或$C_{old,new}$，这取决于获胜的candidate是否收到了$C_{old,new}$。无论如何，$C_{new}$在此期间不能做出单方面的决定。

一旦$C_{old,new}$被提交，$C_{old}$和$C_{new}$都不能在未经对方同意的情况下做出决定，leader completeness属性确保只有带有$C_{old,new}$条目的服务器才能选为leader。现在leader可以安全地创建描述$C_{new}$的日志条目并将其复制到集群中。同样，这个配置将在每个服务器上立即生效。当新配置按照$C_{new}$规则提交时，旧配置无效，不再新配置中的服务器可以关闭。$C_{old}$和$C_{new}$不同时做出单边决策，保证安全。

对于重新配置，有三个问题需要解决。第一个问题是：新服务器最初可能不存储任何日志条目。如果在这种状态下将它们添加到集群中，它们可能需要很长时间才能跟上，在此期间可能无法提交新的日志条目。为了避免可用性差距，Raft在配置更改之前引入一个额外的阶段，在这个阶段中，新服务器作为没有投票权的成员加入集群（leader向他们复制日志条目，但它们不是大多数）。一旦新服务器跟上了集群的其余部分，就可以按照上面描述的那样进行重新配置。

第二个问题是集群leader可能不是新配置的一部分。在这种情况下，一旦提交了$C_{new}$日志条目，leader就会转换为follower状态。这意味着有一段时间（在它提交$C_{new}$时），leader管理一个不包括自己的集群，它复制日志条目，但不计算自己在大多数中。在$C_{new}$提交时，leader进行转换，因为这是新配置可以独立运行的第一个点（总是从$C_{new}$中选择leader）。在此之前，可能只有$C_{old}$的服务器可以被选为leader。

第三个问题是删除的服务器（不在$C_{new}$中的服务器）可能会破坏集群。这些服务器将不会接受心跳，因此它们将超时并开始新的选举。然后它们发送RequestVote RPC与新的term编号，这将导致当前的leader恢复到follower状态。最终会选出一个新的leader，但是被移除的服务器会再次超时，这个过程会重复，导致可用性差。

为了防止这个问题，当服务器认为当前的leader存在时，会忽略RequestVote RPC。具体来说，如果服务器在最小选举超时时间内受到RequestVote RPC，不会更新它term或授予它的投票。这并不影响正常的选举，在正常的选举中，每个服务器在开始选举之前至少等待一个选举超时时间。然而，这有助于避免被移除的服务器造成的中断：如果一个leader能够它的集群的心跳，那么它就不会被更大的term取代。

## 日志压缩

Raft的日志在正常操作期间增长，以合并更多的客户端请求，但在实际系统中，不能无限制地增长。随着日志的增长，会占用更多的空间，需要更多的时间来重放。如果没有某种机制来丢弃累积在日志中的过时信息，这将导致可用性问题。

快照是最简单的压缩方法。在创建快照时，将整个当前系统状态写入稳定存储上的快照，然后丢弃到此点的整个日志。

增量压缩也是一种日志压缩方法，如日志清理和日志结构的合并树。它们一次对数据的一部分进行操作，因此它们会随着时间的推移更均匀地分配压缩负载。首先选择一个积累了许多以删除和覆盖对象的数据区域，然后更紧凑地重写该区域的活动对象并释放该区域。与快照相比，这需要更多的机制和复杂性，快照总是对整个数据集进行操作，从而简化了问题。虽然日志清理需要修改Raft，但状态机可以使用与快照相同接口的LSM树。

Raft快照的基本思想是每个服务器都独立地获取快照，仅覆盖其日志中提交的条目。大部分工作由状态机将其当前状态写入快照组成。Raft还在快照中包含少量元数据：最后包含的索引是快照替换的日志中最后一个条目的索引（状态机应用的最后一个条目），最后包含的term是这个条目的term。保留这些是为了支持对快照之后的第一个日志条目进行AppendEntries一致性检查，因为该条目需要以前的日志索引和术语。为了启用集群成员关系更改，快照还在日志中包含最近包含的索引的最新配置。一旦服务器完成了快照的写入，可能会删除所有的日志条目，直到最后包含的索引、以及之前的快照。

虽然服务器通常独立拍摄快照，但leader必须偶尔向落后的follower发送快照。当leader已经丢弃了需要发送给follower的下一个日志条目时，就会发生这种情况。幸运的是，这种情况在正常操作中是不太可能发生的：与领先者保持一致的follower可能已经有了这个条目。但是异常慢的follower或加入集群的新服务器不会。让这样一个follower快速跟上的方法是leader通过网络发送一个快照。

leader使用一个`InstallSnapshot`的RPC将快照发送给远远落后的follower。当follower接收到使用此RPC的快照时，必须决定如何处理其现有的日志条目。通常，快照将包含收件人日志中尚未包含的新信息。在这种情况下，follower丢弃它的整个日志，全部被快照取代，并且可能有与快照冲突的未提交条目。如果follower收到一个描述其日志前缀的快照（由于重传或错误），那么快照覆盖的日志条目将被删除，但是快照后面的条目仍然有效，必须保留。

InstallSnapshot RPC的摘要（快照被分割成块用于传输，leader总是按顺序发送大块，给予follower每个区块生命符号类似于心跳，可以重置它的选举计时器）：

- 参数：
  - term：leader的term
  - leaderId：follower可以重定向的client
  - lastIncludedIndex：快照替换从前到此索引的所有题目
  - lastIncludedTerm：lastIncludedIndex所在的term
  - offset：数据块在快照文件中位置的字节偏移量
  - data[]：快照块的原始字节，从偏移量开始
  - done：如果这是最后一块，则为true
- 回复：
  - term：当前term，让leader更新自己
- 接收机实现：
  1. 如果term<currentTerm，立即回复
  2. 如果收到第一个区块（偏移为0）就创建新的文件
  3. 按照给定偏移量将数据写入快照文件
  4. 如果done为false，则回复并等待更多的数据块
  5. 保存快照文件，丢弃索引较小的现有快照或部分快照
  6. 如果现有日志条目与快照最后包含的条目具有相同的索引和term，则保留它后面的日志条目并进行回复
  7. 丢弃整个日志
  8. 使用快照内容重置状态机（并加载快照的集群配置）

这种快照方法背离了Raft的强leader原则，因为follower可以在leader不知情的情况下拍摄快照。然而，可以认为这种情况是合理的。虽然一个leader有助于避免冲突的决定达成共识，但共识已经达成时，快照中没有冲突的决定。数据仍然是只从leader流向follower，只是follower现在可以重新组织他们的数据。

考虑另一种基于leader的方法，其中只有leader会创建一个快照，然后会将这个快照发送给它的每个追随者。然而，这有两个缺点。

- 将快照发送给每个follower会浪费网络带宽并减慢快照过程。每个follower都已经拥有了生成自己快照所需的信息，对于服务器来说，从其本地状态生成快照通常比通过网络发送和接收快照成本更小。
- leader的实现将更加复杂。例如，leader将需要向追随者发送快照，同时向它们复制新的日志条目，以避免阻止新的客户端请求。

还有两个问题会影响快照性能。首先，服务器必须决定何时进行快照。如果服务器过于频繁地快照，会浪费磁盘带宽和能量；如果快照的频率太低，可能会耗尽存储容量，并且会增加重启期间重放日志所需的时间。一个简单的策略是在日志达到固定字节大小时进行快照。如果将这个大小设置为明显大于快照的预期大小，那么快照的磁盘带宽开销将会很小。

第二个性能问题是：写快照可能会花费大量时间，不希望这会延迟正常操作。解决方案是使用COW技术，以便在不影响写入快照的情况下接收新的更新。例如，用函数式数据结构构件的状态机自然支持这一点。或者操作系统的写时复制可以用来创建整个状态机的内存快照。

## 客户端交互

Raft的客户端将所有请求发送给leader。当客户端第一次启动时，它连接到一个随机选择的服务器。如果客户机的第一选择不是leader，那么服务器将拒绝客户端的请求，并提供关于最近收到的leader的信息（AppendEntries请求包括leader的网络地址）。如果leader崩溃，客户端请求超时；然后客户端用随机选择的服务器再次尝试。

Raft的目标是实现线性化的语义（每个操作在调用和响应之间的某个点上只执行一次）。然而，Raft可以多次执行命令。解决方案是客户端为每个命令分配唯一的序列号。然后，状态机跟踪为每个客户端处理的最新序列号以及相关的响应。如果它接收到一个序列号已经被执行过的命令，会立即响应，而不会重新执行请求。

可以在不向日志中写入任何内容的情况下处理只读操作。但是，如果没有额外的措施，就会有返回过时数据的风险，因为响应请求的leader可能不知道它被新leader所取代。线性化的读取必须不返回过时的数据，Raft需要两个额外的预防措施来保证这一点，而无需使用日志。首先，leader必须拥有提交条目的最新信息。leader completeness属性保证一个leader拥有所有提交的条目，但是在term开始时，可能不知道哪些是提交的条目。为了找到答案，需要提交一个来自其term的条目。Raft通过让每个leader在其term开始时向日志中提交一个空白的无操作条目来处理这个问题。第二，一个leader必须在处理一个只读请求之前检查它是否已经被废弃（如果最近的leader已经被选举出来，它的信息可能已经过时）。Raft通过让leader在响应只读请求之前与大部分集群交换心跳信息来处理这个问题。或者leader可以依赖心跳机制来提供一种形式的租赁，但这将依赖于定时来确保安全（假定有限度的时钟偏差）。

